{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:orange;font-size:20pt;font-weight:bold\">CS412 - Machine Learning - Term Project Notebook - \"MLCoders\"</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-size:14pt;\">This Jupyter notebook contains the necessary code blocks for both importing, cleaning, vectorizing and training the dataset and model as well as the EDA parts for the sake of gathering results and giving clear recommendations.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:red;font-weight:bold\">Required Libraries</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, for classification, tokenization and parsing, we have got help from the leading human language data platform NLTK (Natural Language Toolkit). It helped us to better classify our dataset by using their wordnet and stopwords dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/beyzabalota/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/beyzabalota/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here, we downloaded the NLTK data.\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the download, we imported our dataset, and initialized the lemmetizer from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bug_id</th>\n",
              "      <th>summary</th>\n",
              "      <th>severity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>365569</td>\n",
              "      <td>Remove workaround from bug 297227</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>365578</td>\n",
              "      <td>Print Preview crashes on any URL in gtk2 builds</td>\n",
              "      <td>critical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>365582</td>\n",
              "      <td>Lines are not showing in table</td>\n",
              "      <td>major</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>365584</td>\n",
              "      <td>Firefox render ÛÏsimplified ArabicÛ font fa...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>365597</td>\n",
              "      <td>Crash [@ nsINodeInfo::NodeInfoManager]</td>\n",
              "      <td>critical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159993</th>\n",
              "      <td>1143381</td>\n",
              "      <td>block elements with height after float left or...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159994</th>\n",
              "      <td>1143392</td>\n",
              "      <td>typing in google translate will send reset inp...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159995</th>\n",
              "      <td>1143394</td>\n",
              "      <td>[gstreamer] Nightly instantly crashes on Youtu...</td>\n",
              "      <td>critical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159996</th>\n",
              "      <td>1143395</td>\n",
              "      <td>Right click on Flash object with accessibility...</td>\n",
              "      <td>critical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159997</th>\n",
              "      <td>1143398</td>\n",
              "      <td>OSX Javascript new Date() initialized with Sta...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159998 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         bug_id                                            summary  severity\n",
              "0        365569                  Remove workaround from bug 297227    normal\n",
              "1        365578    Print Preview crashes on any URL in gtk2 builds  critical\n",
              "2        365582                     Lines are not showing in table     major\n",
              "3        365584  Firefox render ÛÏsimplified ArabicÛ font fa...    normal\n",
              "4        365597             Crash [@ nsINodeInfo::NodeInfoManager]  critical\n",
              "...         ...                                                ...       ...\n",
              "159993  1143381  block elements with height after float left or...    normal\n",
              "159994  1143392  typing in google translate will send reset inp...    normal\n",
              "159995  1143394  [gstreamer] Nightly instantly crashes on Youtu...  critical\n",
              "159996  1143395  Right click on Flash object with accessibility...  critical\n",
              "159997  1143398  OSX Javascript new Date() initialized with Sta...    normal\n",
              "\n",
              "[159998 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Count of Each Unique Value in 'severity' Column:\n",
            "severity\n",
            "normal         125854\n",
            "critical        18658\n",
            "major            6053\n",
            "enhancement      4426\n",
            "minor            3102\n",
            "trivial          1204\n",
            "blocker           701\n",
            "Name: count, dtype: int64\n",
            "(159998, 3)\n"
          ]
        }
      ],
      "source": [
        "train_file_path = 'bugs-train.csv'  # Loaded the dataset.\n",
        "data = pd.read_csv(train_file_path)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "display(data)\n",
        "\n",
        "if 'severity' in data.columns:\n",
        "    print(\"\\nCount of Each Unique Value in 'severity' Column:\")\n",
        "    print(data['severity'].value_counts())\n",
        "    \n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After learning more information about our dataset, we wrote a function to preprocess the wanted texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\d', ' ', text)\n",
        "    \n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we applied this function to the summary and severity columns of our dataset, to get a better learning result afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply text preprocessing to the summary column\n",
        "data['cleaned_summary'] = data['summary'].apply(preprocess_text)\n",
        "\n",
        "# Label Encoding for the 'severity' column\n",
        "label_encoder = LabelEncoder()\n",
        "data['severity_encoded'] = label_encoder.fit_transform(data['severity'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To vectorize the data, we have used TF-IDF (Term Frequency-Inverse Document Frequency) vectorization. This helped us to understand which words are more important then others by looking at their count of appearance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Used TF-IDF vectorization with maximum of 5000 most frequent words.\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_summary'])\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out()) # Converting features to dataframe\n",
        "\n",
        "processed_data = pd.concat([tfidf_df, data['severity_encoded']], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now is the time for training! For this, we split the training data to training and testing sets in a 80 to 20 fashion with the newly vectorized columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the training data:\n",
            "         __  _____  ________  __exposedprops__  __int  __proto__   _a  \\\n",
            "138894  0.0    0.0       0.0               0.0    0.0        0.0  0.0   \n",
            "115279  0.0    0.0       0.0               0.0    0.0        0.0  0.0   \n",
            "129928  0.0    0.0       0.0               0.0    0.0        0.0  0.0   \n",
            "74987   0.0    0.0       0.0               0.0    0.0        0.0  0.0   \n",
            "36702   0.0    0.0       0.0               0.0    0.0        0.0  0.0   \n",
            "\n",
            "        _cairo_d  _demuxer  _max  ...  zombie  zone  zoom  zoomed  zooming  \\\n",
            "138894       0.0       0.0   0.0  ...     0.0   0.0   0.0     0.0      0.0   \n",
            "115279       0.0       0.0   0.0  ...     0.0   0.0   0.0     0.0      0.0   \n",
            "129928       0.0       0.0   0.0  ...     0.0   0.0   0.0     0.0      0.0   \n",
            "74987        0.0       0.0   0.0  ...     0.0   0.0   0.0     0.0      0.0   \n",
            "36702        0.0       0.0   0.0  ...     0.0   0.0   0.0     0.0      0.0   \n",
            "\n",
            "        zwj  zwnj   û_   ûª  ûªt  \n",
            "138894  0.0   0.0  0.0  0.0  0.0  \n",
            "115279  0.0   0.0  0.0  0.0  0.0  \n",
            "129928  0.0   0.0  0.0  0.0  0.0  \n",
            "74987   0.0   0.0  0.0  0.0  0.0  \n",
            "36702   0.0   0.0  0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 5000 columns]\n",
            "138894    5\n",
            "115279    3\n",
            "129928    5\n",
            "74987     5\n",
            "36702     3\n",
            "Name: severity_encoded, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X = processed_data.drop('severity_encoded', axis=1)\n",
        "y = processed_data['severity_encoded']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"First few rows of the training data:\")\n",
        "print(X_train.head())\n",
        "print(y_train.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the splitting, we have trained our selected model xgboost. Extreme gradient boosting is especially helpful for supervised learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.85984375\n",
            "Macro Average Precision Score: 0.8572595124084073\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "# We trained with the selected parameters, as we found these as the optimal ones from our grid search\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 6,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'learning_rate': 0.1, # Used this to prevent overfitting\n",
        "    'max_depth': 6,  \n",
        "    'n_estimators': 100,  \n",
        "    'subsample': 0.8,  \n",
        "    'colsample_bytree': 0.8, \n",
        "    'gamma': 0,  \n",
        "    'min_child_weight': 1  \n",
        "}\n",
        "\n",
        "# Training part\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Made the predictions\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# The accuracy:\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# And the macro precision\n",
        "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
        "print(f\"Macro Average Precision Score: {macro_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the training of our model, we are continuing with the part of testing it and saving the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_file_path = 'bugs-test.csv'  # Our test data\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "test_data['cleaned_summary'] = test_data['summary'].apply(preprocess_text)\n",
        "tfidf_features_test = tfidf_vectorizer.transform(test_data['cleaned_summary'])\n",
        "tfidf_df_test = pd.DataFrame(tfidf_features_test.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "test_predictions = xgb_model.predict(tfidf_df_test)\n",
        "predicted_severity = label_encoder.inverse_transform(test_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To finish the prediction and model training part, we are writing the submissions to a new file, which we used to upload to Kaggle competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to 'submission.csv'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "submission_df = pd.DataFrame({ 'bug_id': test_data['bug_id'], 'severity': predicted_severity })\n",
        "\n",
        "submission_df.to_csv('submissionvol5.csv', index=False)\n",
        "print(\"Predictions saved to 'submission.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This marks the end of the training part."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
